{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# loading 60.000 rows of training data from MNIST\n",
    "df_mnist = pd.read_csv(u'data/mnist_train.csv', sep=',', header=None)\n",
    "\n",
    "# distributing the data as features & target\n",
    "features = np.array(df_mnist.iloc[:, [i+1 for i in range(784)]])\n",
    "target = np.array(df_mnist.iloc[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N is batch size, getting N rows in every iteration; D_in is input dimension 28x28=784;\n",
    "# H is hidden dimension, we decide; D_out is output dimension, which is 1.\n",
    "N, D_in, H, D_out = 100, 784, 28, 1\n",
    "\n",
    "# defining the functions to be used between our layers\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "# defining the loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss(reduce=True, size_average=False)\n",
    "\n",
    "# eta value of this regression\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# defining optimizer algorithm\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# starting epochs\n",
    "for t in range(600):\n",
    "    # selecting N rows for every epoch\n",
    "    idx = np.random.choice(len(x), size=N, replace=True)\n",
    "    \n",
    "    # fetching the selected data as x & y\n",
    "    x = torch.FloatTensor(features[idx,:])\n",
    "    y = torch.LongTensor(target[idx])\n",
    "    \n",
    "    # calculating y_pred by passing fetched x to our model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # comparing calculated y_pred and actual y\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    \n",
    "    # printing the loss in every 100 epochs\n",
    "    if(t%100 == 0):\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, our model is trainde, it is time for 10.000 rows of test data\n",
    "df_mnist_test = pd.read_csv(u'mnist_test.csv', sep=',', header=None)\n",
    "\n",
    "# distributing the test data as features_test & target_test\n",
    "x_test = torch.FloatTensor(np.array(df_mnist_test.iloc[:, [i+1 for i in range(784)]]))\n",
    "y_test = torch.LongTensor(np.array(df_mnist_test.iloc[:, 0]))\n",
    "\n",
    "# getting the result of the test data from the trained model\n",
    "result = torch.round(model(x_test))\n",
    "        \n",
    "# building a confusion matrix\n",
    "confusion_matrix = torch.zeros((10, 10))\n",
    "\n",
    "for i in range(len(result)):\n",
    "    confusion_matrix[int(y_test[i]), int(result[i])] += 1\n",
    "    \n",
    "print(\"Confusion matrix:\\n\", confusion_matrix, \"\\n\")\n",
    "    \n",
    "accuracy = torch.sum(torch.diag(confusion_matrix))/torch.sum(confusion_matrix)\n",
    "\n",
    "print(\"Accuracy:\", accuracy*100, \"%\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
