{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IRIS Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading 150 rows of data from IRIS Dataset\n",
    "df_iris = pd.read_csv(u'data/iris.txt',sep=' ')\n",
    "\n",
    "# distributing the data as features & target ('c' column is the target, others are the features)\n",
    "features, target = np.array(df_iris[['sl','sw','pl','pw']]), np.array(df_iris['c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 170.591064453125\n",
      "50 70.36634826660156\n",
      "100 19.250059127807617\n",
      "150 10.490957260131836\n",
      "200 8.667634963989258\n",
      "250 7.878386974334717\n",
      "300 7.43941068649292\n",
      "350 7.1361775398254395\n",
      "400 6.900533676147461\n",
      "450 6.706483364105225\n",
      "499 6.546168327331543\n"
     ]
    }
   ],
   "source": [
    "# D_in is input dimension 4;\n",
    "# H1 is the first hidden layer dimension, we decide;\n",
    "# H2 is the second hidden layer dimension, we decide;\n",
    "# D_out is output dimension, which is 3 in this case.\n",
    "# CrossEntropyLoss returns the possibility of the result being a member of each possible class.\n",
    "# In this case, the output of any row will be like this [0.2 0.7 0.1] (20% Class 1, 70% Class 2, 10% Class 3)\n",
    "D_in, H1, H2, D_out = 4, 16, 8, 3\n",
    "\n",
    "# defining the functions to be used between our layers\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H1),\n",
    "    torch.nn.ReLU(), # ReLU function optimizes output by setting negative results as 0\n",
    "    torch.nn.Linear(H1, H2),\n",
    "    torch.nn.ReLU(),    \n",
    "    torch.nn.Linear(H2, D_out),\n",
    ")\n",
    "\n",
    "# defining the loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "# eta value of this regression\n",
    "learning_rate = 0.005\n",
    "\n",
    "# defining optimizer algorithm\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# defining epoch limit\n",
    "max_epoch = 500\n",
    "\n",
    "# defining epoch step to print loss\n",
    "print_epoch = 50\n",
    "\n",
    "# converting features and target into tensors\n",
    "x, y = torch.FloatTensor(features), torch.LongTensor(target)\n",
    "    \n",
    "y = y - 1 # Loss function needs the classes starting from 0\n",
    "\n",
    "# starting epochs\n",
    "for t in range(max_epoch): \n",
    "    # calculating y_pred by passing x tensor to our model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # comparing calculated y_pred and actual y\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    \n",
    "    # printing the loss in every print_epoch epochs and at the last\n",
    "    if(t%print_epoch == 0 or t+1 == max_epoch):\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # before the backward pass, zero all of the gradients for the variables it will update\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # update optimizer parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result:\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3] \n",
      "\n",
      "Confusion matrix:\n",
      " tensor([[50.,  0.,  0.],\n",
      "        [ 0., 49.,  1.],\n",
      "        [ 0.,  1., 49.]]) \n",
      "\n",
      "Accuracy: tensor(98.6667) %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# getting the result of the dataset with final model\n",
    "results = model(x)\n",
    "\n",
    "# preparing a 1-layer result array by geting the highest possible result (for [0.2 0.7 0.1] it is 1st index)\n",
    "result = np.array([row.argmax() for row in results])\n",
    "\n",
    "result = result + 1 # incrementing class indexes again for 0th class to be 1\n",
    "\n",
    "print(\"Result:\\n\", result, \"\\n\")\n",
    "\n",
    "# building a n-by-n confusion matrix where n is the number of output classes\n",
    "confusion_matrix = torch.zeros((D_out, D_out))\n",
    "\n",
    "y = y + 1 # incrementing class indexes again for 0th class to be 1\n",
    "\n",
    "# filling up the confusion matrix\n",
    "for i in range(len(result)):\n",
    "    confusion_matrix[int(y[i]) - 1, int(result[i]) - 1] += 1\n",
    "    \n",
    "print(\"Confusion matrix:\\n\", confusion_matrix, \"\\n\")\n",
    "    \n",
    "# calculating model accuracy\n",
    "accuracy = torch.sum(torch.diag(confusion_matrix))/torch.sum(confusion_matrix)\n",
    "\n",
    "print(\"Accuracy:\", accuracy*100, \"%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading 60.000 rows of training data from MNIST Dataset\n",
    "df_mnist = pd.read_csv(u'data/mnist_train.csv', sep=',', header=None)\n",
    "\n",
    "# distributing the data as features & target ([0]th column is the target, [1-784] are the features)\n",
    "features, target = np.array(df_mnist.iloc[:, [i+1 for i in range(784)]]), np.array(df_mnist.iloc[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 681039.75\n",
      "100 15979.205078125\n",
      "200 10977.0087890625\n",
      "300 8616.55078125\n",
      "400 7155.63671875\n",
      "500 6087.1015625\n",
      "600 5313.93798828125\n",
      "700 4673.34912109375\n",
      "800 4210.1640625\n",
      "900 3728.392333984375\n",
      "999 3393.701416015625\n"
     ]
    }
   ],
   "source": [
    "# D_in is input dimension 28*28=784;\n",
    "# H1 is the first hidden layer dimension, we decide;\n",
    "# H2 is the second hidden layer dimension, we decide;\n",
    "# D_out is output dimension, which is 10 (0 to 9) in this case.\n",
    "# CrossEntropyLoss returns the possibility of the result being a member of each possible class.\n",
    "# In this case, the output of any row will be like this:\n",
    "# [.02 .7 .01 .01 .01 .03 .02 .01 .01 .05] (2% number 1, 70% number 2, 1% number 3 etc.)\n",
    "D_in, H1, H2, D_out = 784, 64, 32, 10\n",
    "\n",
    "# defining the functions to be used between our layers\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H1),\n",
    "    torch.nn.ReLU(), # ReLU function optimizes output by setting negative results as 0\n",
    "    torch.nn.Linear(H1, H2),\n",
    "    torch.nn.ReLU(),    \n",
    "    torch.nn.Linear(H2, D_out),\n",
    ")\n",
    "\n",
    "# defining the loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "# eta value of this regression\n",
    "learning_rate = 0.005\n",
    "\n",
    "# defining optimizer algorithm\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# defining epoch limit\n",
    "max_epoch = 1000\n",
    "\n",
    "# defining epoch step to print loss\n",
    "print_epoch = 100\n",
    "\n",
    "# converting features and target into tensors\n",
    "x, y = torch.FloatTensor(features), torch.LongTensor(target)\n",
    "\n",
    "# starting epochs\n",
    "for t in range(max_epoch): \n",
    "    # calculating y_pred by passing x tensor to our model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # comparing calculated y_pred and actual y\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    \n",
    "    # printing the loss in every print_epoch epochs and at the last\n",
    "    if(t%print_epoch == 0 or t+1 == max_epoch):\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # before the backward pass, zero all of the gradients for the variables it will update\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # update optimizer parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result:\n",
      " [7 2 1 ... 4 5 6] \n",
      "\n",
      "Confusion matrix:\n",
      " tensor([[ 962.,    1.,    1.,    0.,    1.,    2.,    4.,    0.,    6.,    3.],\n",
      "        [   0., 1116.,    3.,    2.,    2.,    1.,    3.,    2.,    5.,    1.],\n",
      "        [   8.,    4.,  973.,   13.,    5.,    4.,    6.,    6.,   12.,    1.],\n",
      "        [   1.,    1.,    8.,  948.,    0.,   18.,    1.,    7.,   16.,   10.],\n",
      "        [   0.,    0.,   11.,    0.,  925.,    2.,   10.,    7.,    2.,   25.],\n",
      "        [   6.,    6.,    1.,   21.,    5.,  826.,   11.,    2.,   10.,    4.],\n",
      "        [   9.,    2.,    5.,    1.,    5.,    9.,  922.,    0.,    4.,    1.],\n",
      "        [   2.,    5.,   12.,   10.,   10.,    3.,    1.,  956.,    3.,   26.],\n",
      "        [   6.,    7.,   10.,   21.,    6.,   13.,    5.,    5.,  893.,    8.],\n",
      "        [   7.,    6.,    0.,    8.,   20.,    3.,    3.,   13.,    8.,  941.]]) \n",
      "\n",
      "Accuracy: tensor(94.6200) %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# now, our model is trained, it is time for 10.000 rows of test data to test the final model\n",
    "df_mnist_test = pd.read_csv(u'data/mnist_test.csv', sep=',', header=None)\n",
    "\n",
    "# distributing the test data as x_test & y_test\n",
    "x_test, y_test = torch.FloatTensor(np.array(df_mnist_test.iloc[:, [i+1 for i in range(784)]])), torch.LongTensor(np.array(df_mnist_test.iloc[:, 0]))\n",
    "\n",
    "# getting the 10-layer results of the test data from the trained model\n",
    "results = model(x_test)\n",
    "\n",
    "# preparing a 1-layer result array by geting the highest possible result\n",
    "result = np.array([row.argmax() for row in results])\n",
    "\n",
    "print(\"Result:\\n\", result, \"\\n\")\n",
    "\n",
    "# building a n-by-n confusion matrix where n is the number of output classes\n",
    "confusion_matrix = torch.zeros((D_out, D_out))\n",
    "\n",
    "# filling up the confusion matrix\n",
    "for i in range(len(result)):\n",
    "    confusion_matrix[int(y_test[i]), int(result[i])] += 1\n",
    "    \n",
    "print(\"Confusion matrix:\\n\", confusion_matrix, \"\\n\")\n",
    "    \n",
    "# calculating model accuracy\n",
    "accuracy = torch.sum(torch.diag(confusion_matrix))/torch.sum(confusion_matrix)\n",
    "\n",
    "print(\"Accuracy:\", accuracy*100, \"%\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
